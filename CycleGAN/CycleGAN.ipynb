{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CycleGAN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMTWpNDJ1GAMNo4mNRVun5l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hphp777/GAN/blob/master/CycleGAN/CycleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Dataset"
      ],
      "metadata": {
        "id": "-kzhr8dA-Edv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcUA_qXZ57Bl"
      },
      "outputs": [],
      "source": [
        "!mkdir datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FILE= \"apple2orange\"\n",
        "\n",
        "URL= \"https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/\" + FILE + \".zip\"\n",
        "ZIP_FILE= \"./datasets/\" + FILE + \".zip\"\n",
        "TARGET_DIR= \"./datasets/\" + FILE + \"/\""
      ],
      "metadata": {
        "id": "XsZ5z1yC8XgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/apple2orange.zip -O ./datasets/apple2orange.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHdkJ-RZ677Y",
        "outputId": "2fc65077-e082-4972-b84b-fa4136fa0889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-08 06:08:13--  http://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/apple2orange.zip\n",
            "Resolving people.eecs.berkeley.edu (people.eecs.berkeley.edu)... 128.32.244.190\n",
            "Connecting to people.eecs.berkeley.edu (people.eecs.berkeley.edu)|128.32.244.190|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 78456409 (75M) [application/zip]\n",
            "Saving to: ‘./datasets/apple2orange.zip’\n",
            "\n",
            "./datasets/apple2or 100%[===================>]  74.82M  1.76MB/s    in 43s     \n",
            "\n",
            "2022-02-08 06:08:56 (1.72 MB/s) - ‘./datasets/apple2orange.zip’ saved [78456409/78456409]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./datasets/apple2orange/\n",
        "!unzip ./datasets/apple2orange.zip -d ./datasets/\n",
        "!rm ./datasets/apple2orange.zip"
      ],
      "metadata": {
        "id": "nAthdK5R8Ncn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drive Mount and Package Install"
      ],
      "metadata": {
        "id": "nlJR7x_9R0qL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NV7MDNo89plD",
        "outputId": "5abb4b28-fd2a-41a4-b1a5-12550c63aa9a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrFTNypGBQzd",
        "outputId": "cd993f3e-b295-4875-a7be-24a021060b2e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-dcju0d5r\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-dcju0d5r\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.7.0)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-py3-none-any.whl size=101077 sha256=74366118bc9ac4f48d4c75d0f7888b47c235e8de485956cdbbaf4b68d17b154f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ge1bswlh/wheels/bb/1f/f2/b57495012683b6b20bbae94a3915ec79753111452d79886abc\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scipy==1.1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "XQTfY9sNDraI",
        "outputId": "5ae11635-149f-4a70-b92f-f4e24e43b7eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy==1.1.0\n",
            "  Downloading scipy-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (31.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 31.2 MB 46.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.1.0) (1.19.5)\n",
            "Installing collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pymc3 3.11.4 requires scipy>=1.2.0, but you have scipy 1.1.0 which is incompatible.\n",
            "plotnine 0.6.0 requires scipy>=1.2.0, but you have scipy 1.1.0 which is incompatible.\n",
            "jax 0.2.25 requires scipy>=1.2.1, but you have scipy 1.1.0 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed scipy-1.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "scipy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Dataset"
      ],
      "metadata": {
        "id": "F-j_yIq3R61V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class DataLoader():\n",
        "    def __init__(self, dataset_name, img_res=(128, 128)):\n",
        "        self.dataset_name = dataset_name\n",
        "        self.img_res = img_res\n",
        "\n",
        "    def load_data(self, domain, batch_size=1, is_testing=False):\n",
        "        data_type = \"train%s\" % domain if not is_testing else \"test%s\" % domain\n",
        "        path = glob('/content/drive/MyDrive/Colab Notebooks/CycleGAN/%s/%s/*' % (self.dataset_name, data_type))\n",
        "\n",
        "        batch_images = np.random.choice(path, size=batch_size)\n",
        "\n",
        "        imgs = []\n",
        "\n",
        "        for img_path in batch_images:\n",
        "            img = self.imread(img_path)\n",
        "            if not is_testing:\n",
        "                img = scipy.misc.imresize(img, self.img_res)\n",
        "\n",
        "                if np.random.random() > 0.5:\n",
        "                    img = np.fliplr(img)\n",
        "            else:\n",
        "                img = scipy.misc.imresize(img, self.img_res)\n",
        "            imgs.append(img)\n",
        "\n",
        "        imgs = np.array(imgs)/127.5 - 1.\n",
        "\n",
        "        return imgs\n",
        "\n",
        "    def load_batch(self, batch_size=1, is_testing=False):\n",
        "        data_type = \"train\" if not is_testing else \"val\"\n",
        "        path_A = glob('/content/drive/MyDrive/Colab Notebooks/CycleGAN/%s/%sA/*' % (self.dataset_name, data_type))\n",
        "        path_B = glob('/content/drive/MyDrive/Colab Notebooks/CycleGAN/%s/%sB/*' % (self.dataset_name, data_type))\n",
        "\n",
        "        self.n_batches = int(min(len(path_A), len(path_B)) / batch_size)\n",
        "        total_samples = self.n_batches * batch_size\n",
        "\n",
        "        # Sample n_batches * batch_size from each path list so that model sees all\n",
        "        # samples from both domains\n",
        "        path_A = np.random.choice(path_A, total_samples, replace=False)\n",
        "        path_B = np.random.choice(path_B, total_samples, replace=False)\n",
        "\n",
        "        for i in range(self.n_batches-1):\n",
        "            batch_A = path_A[i*batch_size:(i+1)*batch_size]\n",
        "            batch_B = path_B[i*batch_size:(i+1)*batch_size]\n",
        "            imgs_A, imgs_B = [], []\n",
        "            for img_A, img_B in zip(batch_A, batch_B):\n",
        "                img_A = self.imread(img_A)\n",
        "                img_B = self.imread(img_B)\n",
        "\n",
        "                img_A = scipy.misc.imresize(img_A, self.img_res)\n",
        "                img_B = scipy.misc.imresize(img_B, self.img_res)\n",
        "\n",
        "                if not is_testing and np.random.random() > 0.5:\n",
        "                        img_A = np.fliplr(img_A)\n",
        "                        img_B = np.fliplr(img_B)\n",
        "\n",
        "                imgs_A.append(img_A)\n",
        "                imgs_B.append(img_B)\n",
        "\n",
        "            imgs_A = np.array(imgs_A)/127.5 - 1.\n",
        "            imgs_B = np.array(imgs_B)/127.5 - 1.\n",
        "\n",
        "            yield imgs_A, imgs_B\n",
        "\n",
        "    def load_img(self, path):\n",
        "        img = self.imread(path)\n",
        "        img = scipy.misc.imresize(img, self.img_res)\n",
        "        img = img/127.5 - 1.\n",
        "        return img[np.newaxis, :, :, :]\n",
        "\n",
        "    def imread(self, path):\n",
        "        return scipy.misc.imread(path, mode='RGB').astype(np.float)"
      ],
      "metadata": {
        "id": "lKXTBHXQ_3A5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "v4NETeq7-_BR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "import scipy\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class CycleGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 128\n",
        "        self.img_cols = 128\n",
        "        self.channels = 3\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "\n",
        "        # Configure data loader\n",
        "        self.dataset_name = 'apple2orange'\n",
        "        self.data_loader = DataLoader(dataset_name=self.dataset_name,\n",
        "                                      img_res=(self.img_rows, self.img_cols))\n",
        "\n",
        "\n",
        "        # Calculate output shape of D (PatchGAN)\n",
        "        patch = int(self.img_rows / 2**4)\n",
        "        self.disc_patch = (patch, patch, 1)\n",
        "\n",
        "        # Number of filters in the first layer of G and D\n",
        "        self.gf = 32\n",
        "        self.df = 64\n",
        "\n",
        "        # Loss weights\n",
        "        self.lambda_cycle = 10.0                    # Cycle-consistency loss\n",
        "        self.lambda_id = 0.1 * self.lambda_cycle    # Identity loss\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminators\n",
        "        self.d_A = self.build_discriminator()\n",
        "        self.d_B = self.build_discriminator()\n",
        "        self.d_A.compile(loss='mse',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "        self.d_B.compile(loss='mse',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        #-------------------------\n",
        "        # Construct Computational\n",
        "        #   Graph of Generators\n",
        "        #-------------------------\n",
        "\n",
        "        # Build the generators\n",
        "        self.g_AB = self.build_generator()\n",
        "        self.g_BA = self.build_generator()\n",
        "\n",
        "        # Input images from both domains\n",
        "        img_A = Input(shape=self.img_shape)\n",
        "        img_B = Input(shape=self.img_shape)\n",
        "\n",
        "        # Translate images to the other domain\n",
        "        fake_B = self.g_AB(img_A)\n",
        "        fake_A = self.g_BA(img_B)\n",
        "        # Translate images back to original domain\n",
        "        reconstr_A = self.g_BA(fake_B)\n",
        "        reconstr_B = self.g_AB(fake_A)\n",
        "        # Identity mapping of images\n",
        "        img_A_id = self.g_BA(img_A)\n",
        "        img_B_id = self.g_AB(img_B)\n",
        "\n",
        "        # For the combined model we will only train the generators\n",
        "        self.d_A.trainable = False\n",
        "        self.d_B.trainable = False\n",
        "\n",
        "        # Discriminators determines validity of translated images\n",
        "        valid_A = self.d_A(fake_A)\n",
        "        valid_B = self.d_B(fake_B)\n",
        "\n",
        "        # Combined model trains generators to fool discriminators\n",
        "        self.combined = Model(inputs=[img_A, img_B],\n",
        "                              outputs=[ valid_A, valid_B,\n",
        "                                        reconstr_A, reconstr_B,\n",
        "                                        img_A_id, img_B_id ])\n",
        "        self.combined.compile(loss=['mse', 'mse',\n",
        "                                    'mae', 'mae',\n",
        "                                    'mae', 'mae'],\n",
        "                            loss_weights=[  1, 1,\n",
        "                                            self.lambda_cycle, self.lambda_cycle,\n",
        "                                            self.lambda_id, self.lambda_id ],\n",
        "                            optimizer=optimizer)\n",
        "\n",
        "    def build_generator(self):\n",
        "        \"\"\"U-Net Generator\"\"\"\n",
        "\n",
        "        def conv2d(layer_input, filters, f_size=4):\n",
        "            \"\"\"Layers used during downsampling\"\"\"\n",
        "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "            d = LeakyReLU(alpha=0.2)(d)\n",
        "            d = InstanceNormalization()(d)\n",
        "            return d\n",
        "\n",
        "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
        "            \"\"\"Layers used during upsampling\"\"\"\n",
        "            u = UpSampling2D(size=2)(layer_input)\n",
        "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
        "            if dropout_rate:\n",
        "                u = Dropout(dropout_rate)(u)\n",
        "            u = InstanceNormalization()(u)\n",
        "            u = Concatenate()([u, skip_input])\n",
        "            return u\n",
        "\n",
        "        # Image input\n",
        "        d0 = Input(shape=self.img_shape)\n",
        "\n",
        "        # Downsampling\n",
        "        d1 = conv2d(d0, self.gf)\n",
        "        d2 = conv2d(d1, self.gf*2)\n",
        "        d3 = conv2d(d2, self.gf*4)\n",
        "        d4 = conv2d(d3, self.gf*8)\n",
        "\n",
        "        # Upsampling\n",
        "        u1 = deconv2d(d4, d3, self.gf*4)\n",
        "        u2 = deconv2d(u1, d2, self.gf*2)\n",
        "        u3 = deconv2d(u2, d1, self.gf)\n",
        "\n",
        "        u4 = UpSampling2D(size=2)(u3)\n",
        "        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n",
        "\n",
        "        return Model(d0, output_img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        def d_layer(layer_input, filters, f_size=4, normalization=True):\n",
        "            \"\"\"Discriminator layer\"\"\"\n",
        "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "            d = LeakyReLU(alpha=0.2)(d)\n",
        "            if normalization:\n",
        "                d = InstanceNormalization()(d)\n",
        "            return d\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "\n",
        "        d1 = d_layer(img, self.df, normalization=False)\n",
        "        d2 = d_layer(d1, self.df*2)\n",
        "        d3 = d_layer(d2, self.df*4)\n",
        "        d4 = d_layer(d3, self.df*8)\n",
        "\n",
        "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "    def train(self, epochs, batch_size=1, sample_interval=50):\n",
        "\n",
        "        start_time = datetime.datetime.now()\n",
        "\n",
        "        # Adversarial loss ground truths\n",
        "        valid = np.ones((batch_size,) + self.disc_patch)\n",
        "        fake = np.zeros((batch_size,) + self.disc_patch)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n",
        "                # ----------------------\n",
        "                #  Train Discriminators\n",
        "                # ----------------------\n",
        "\n",
        "                # Translate images to opposite domain\n",
        "                fake_B = self.g_AB.predict(imgs_A)\n",
        "                fake_A = self.g_BA.predict(imgs_B)\n",
        "\n",
        "                # Train the discriminators (original images = real / translated = Fake)\n",
        "                dA_loss_real = self.d_A.train_on_batch(imgs_A, valid)\n",
        "                dA_loss_fake = self.d_A.train_on_batch(fake_A, fake)\n",
        "                dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
        "\n",
        "                dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)\n",
        "                dB_loss_fake = self.d_B.train_on_batch(fake_B, fake)\n",
        "                dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
        "\n",
        "                # Total disciminator loss\n",
        "                d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
        "\n",
        "\n",
        "                # ------------------\n",
        "                #  Train Generators\n",
        "                # ------------------\n",
        "\n",
        "                # Train the generators\n",
        "                g_loss = self.combined.train_on_batch([imgs_A, imgs_B],\n",
        "                                                        [valid, valid,\n",
        "                                                        imgs_A, imgs_B,\n",
        "                                                        imgs_A, imgs_B])\n",
        "\n",
        "                elapsed_time = datetime.datetime.now() - start_time\n",
        "\n",
        "                # Plot the progress\n",
        "                print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" \\\n",
        "                                                                        % ( epoch, epochs,\n",
        "                                                                            batch_i, self.data_loader.n_batches,\n",
        "                                                                            d_loss[0], 100*d_loss[1],\n",
        "                                                                            g_loss[0],\n",
        "                                                                            np.mean(g_loss[1:3]),\n",
        "                                                                            np.mean(g_loss[3:5]),\n",
        "                                                                            np.mean(g_loss[5:6]),\n",
        "                                                                            elapsed_time))\n",
        "\n",
        "                # If at save interval => save generated image samples\n",
        "                if batch_i % sample_interval == 0:\n",
        "                    self.sample_images(epoch, batch_i)\n",
        "\n",
        "    def sample_images(self, epoch, batch_i):\n",
        "        os.makedirs('images/%s' % self.dataset_name, exist_ok=True)\n",
        "        r, c = 2, 3\n",
        "\n",
        "        imgs_A = self.data_loader.load_data(domain=\"A\", batch_size=1, is_testing=True)\n",
        "        imgs_B = self.data_loader.load_data(domain=\"B\", batch_size=1, is_testing=True)\n",
        "\n",
        "        # Demo (for GIF)\n",
        "        #imgs_A = self.data_loader.load_img('datasets/apple2orange/testA/n07740461_1541.jpg')\n",
        "        #imgs_B = self.data_loader.load_img('datasets/apple2orange/testB/n07749192_4241.jpg')\n",
        "\n",
        "        # Translate images to the other domain\n",
        "        fake_B = self.g_AB.predict(imgs_A)\n",
        "        fake_A = self.g_BA.predict(imgs_B)\n",
        "        # Translate back to original domain\n",
        "        reconstr_A = self.g_BA.predict(fake_B)\n",
        "        reconstr_B = self.g_AB.predict(fake_A)\n",
        "\n",
        "        gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        titles = ['Original', 'Translated', 'Reconstructed']\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt])\n",
        "                axs[i, j].set_title(titles[j])\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(\"images/%s/%d_%d.png\" % (self.dataset_name, epoch, batch_i))\n",
        "        plt.close()"
      ],
      "metadata": {
        "id": "tz5mCNGy-MmM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Save Images"
      ],
      "metadata": {
        "id": "8Gda-HzxSBRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gan = CycleGAN()"
      ],
      "metadata": {
        "id": "Dp0-voUjAlpm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gan.train(epochs=200, batch_size=1, sample_interval=200)"
      ],
      "metadata": {
        "id": "VXLItcSvGmyF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "39777861-1520-4ea8-cff8-ef96ad368e9d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:77: DeprecationWarning:     `imread` is deprecated!\n",
            "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "    Use ``imageio.imread`` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: DeprecationWarning:     `imresize` is deprecated!\n",
            "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "    Use ``skimage.transform.resize`` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: DeprecationWarning:     `imresize` is deprecated!\n",
            "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "    Use ``skimage.transform.resize`` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mm\n",
            "[Epoch 0/200] [Batch 0/995] [D loss: 28.004910, acc:  23%] [G loss: 91.209991, adv: 37.292962, recon: 0.769981, id: 0.534513] time: 0:00:59.814145 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: DeprecationWarning:     `imresize` is deprecated!\n",
            "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "    Use ``skimage.transform.resize`` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mm\n",
            "[Epoch 0/200] [Batch 1/995] [D loss: 30.911156, acc:  67%] [G loss: 224.024582, adv: 106.785015, recon: 0.472243, id: 0.469745] time: 0:01:06.299915 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 2/995] [D loss: 42.941509, acc:  40%] [G loss: 104.950562, adv: 44.814860, recon: 0.710864, id: 0.503329] time: 0:01:07.104533 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 3/995] [D loss: 25.948049, acc:  35%] [G loss: 18.693281, adv: 1.526111, recon: 0.710540, id: 0.543438] time: 0:01:07.961625 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 4/995] [D loss: 4.207889, acc:  48%] [G loss: 15.601923, adv: 1.312255, recon: 0.582463, id: 0.575597] time: 0:01:09.338014 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 5/995] [D loss: 3.002627, acc:  53%] [G loss: 17.801476, adv: 2.064193, recon: 0.623070, id: 0.488529] time: 0:01:10.419219 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 6/995] [D loss: 1.907557, acc:  48%] [G loss: 8.927279, adv: 1.009438, recon: 0.311094, id: 0.144746] time: 0:01:11.411730 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 7/995] [D loss: 0.467453, acc:  52%] [G loss: 13.379222, adv: 0.730422, recon: 0.536703, id: 0.468950] time: 0:01:12.341662 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 8/995] [D loss: 0.732759, acc:  66%] [G loss: 12.699026, adv: 0.930537, recon: 0.465969, id: 0.644769] time: 0:01:13.315228 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 9/995] [D loss: 0.489580, acc:  55%] [G loss: 12.311902, adv: 0.725288, recon: 0.488548, id: 0.419592] time: 0:01:21.270326 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 10/995] [D loss: 0.391986, acc:  53%] [G loss: 9.986506, adv: 0.923849, recon: 0.347818, id: 0.423985] time: 0:01:21.985039 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 11/995] [D loss: 0.305175, acc:  74%] [G loss: 11.962234, adv: 1.091833, recon: 0.408920, id: 0.670407] time: 0:01:22.528904 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 12/995] [D loss: 0.301558, acc:  85%] [G loss: 11.714747, adv: 0.923681, recon: 0.438972, id: 0.555821] time: 0:01:23.131489 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 13/995] [D loss: 0.897519, acc:  27%] [G loss: 8.199577, adv: 0.426868, recon: 0.334557, id: 0.245280] time: 0:01:23.719459 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 14/995] [D loss: 0.345816, acc:  65%] [G loss: 8.891462, adv: 0.984186, recon: 0.302219, id: 0.254100] time: 0:01:24.331031 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 15/995] [D loss: 0.404027, acc:  57%] [G loss: 13.444448, adv: 1.237935, recon: 0.500641, id: 0.490057] time: 0:01:25.062980 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 16/995] [D loss: 0.495700, acc:  54%] [G loss: 9.044818, adv: 0.860465, recon: 0.327178, id: 0.358217] time: 0:01:25.708069 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 17/995] [D loss: 0.341180, acc:  64%] [G loss: 12.257717, adv: 0.883092, recon: 0.479975, id: 0.364746] time: 0:01:26.210326 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 18/995] [D loss: 0.266313, acc:  79%] [G loss: 12.185018, adv: 1.222109, recon: 0.433774, id: 0.348060] time: 0:01:26.722451 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 19/995] [D loss: 0.416133, acc:  58%] [G loss: 10.433743, adv: 0.838098, recon: 0.386738, id: 0.476765] time: 0:01:27.295149 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 20/995] [D loss: 0.178241, acc:  76%] [G loss: 11.187364, adv: 1.085422, recon: 0.403603, id: 0.392465] time: 0:01:27.889516 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 21/995] [D loss: 0.255764, acc:  69%] [G loss: 9.284615, adv: 0.877068, recon: 0.326137, id: 0.405718] time: 0:01:28.576806 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 22/995] [D loss: 1.069230, acc:  40%] [G loss: 12.828107, adv: 2.277071, recon: 0.373773, id: 0.359446] time: 0:01:29.069211 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 23/995] [D loss: 1.119373, acc:  31%] [G loss: 9.071373, adv: 0.591054, recon: 0.350384, id: 0.367065] time: 0:01:29.564257 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 24/995] [D loss: 0.615076, acc:  51%] [G loss: 10.859401, adv: 1.021626, recon: 0.383352, id: 0.581850] time: 0:01:30.129717 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 25/995] [D loss: 1.615285, acc:  40%] [G loss: 13.101720, adv: 1.685953, recon: 0.436035, id: 0.470433] time: 0:01:30.675500 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 26/995] [D loss: 1.290428, acc:  22%] [G loss: 11.726987, adv: 1.400289, recon: 0.395567, id: 0.602361] time: 0:01:31.235759 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 27/995] [D loss: 0.687534, acc:  59%] [G loss: 10.120033, adv: 0.867620, recon: 0.375027, id: 0.356068] time: 0:01:31.891822 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 28/995] [D loss: 1.318680, acc:  26%] [G loss: 9.440105, adv: 0.644998, recon: 0.364889, id: 0.602754] time: 0:01:32.479734 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 29/995] [D loss: 0.280203, acc:  76%] [G loss: 12.074671, adv: 1.257103, recon: 0.436063, id: 0.259949] time: 0:01:33.129816 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 30/995] [D loss: 0.217032, acc:  87%] [G loss: 9.667542, adv: 0.796500, recon: 0.363657, id: 0.249974] time: 0:01:33.690749 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 31/995] [D loss: 0.350318, acc:  46%] [G loss: 12.182554, adv: 0.900999, recon: 0.461194, id: 0.595525] time: 0:01:34.306412 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 32/995] [D loss: 0.462765, acc:  47%] [G loss: 9.137913, adv: 0.824833, recon: 0.331474, id: 0.247310] time: 0:01:34.860036 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 33/995] [D loss: 0.372382, acc:  62%] [G loss: 5.905224, adv: 1.150059, recon: 0.154677, id: 0.173774] time: 0:01:35.356371 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 34/995] [D loss: 0.302441, acc:  66%] [G loss: 7.734032, adv: 0.862454, recon: 0.263346, id: 0.332600] time: 0:01:35.889307 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 35/995] [D loss: 0.483587, acc:  42%] [G loss: 10.507700, adv: 0.961942, recon: 0.383626, id: 0.465969] time: 0:01:36.479896 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 36/995] [D loss: 0.475099, acc:  57%] [G loss: 10.405313, adv: 0.584811, recon: 0.426131, id: 0.377646] time: 0:01:37.020445 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 37/995] [D loss: 0.811077, acc:  26%] [G loss: 8.228485, adv: 0.644132, recon: 0.306354, id: 0.444921] time: 0:01:37.633404 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 38/995] [D loss: 0.236422, acc:  68%] [G loss: 9.920368, adv: 0.997546, recon: 0.344587, id: 0.485720] time: 0:01:38.314968 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 39/995] [D loss: 0.565576, acc:  46%] [G loss: 11.561908, adv: 0.893806, recon: 0.445432, id: 0.396107] time: 0:01:38.848372 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 40/995] [D loss: 1.083262, acc:  12%] [G loss: 7.485589, adv: 0.560126, recon: 0.291004, id: 0.348408] time: 0:01:39.361744 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 41/995] [D loss: 0.633273, acc:  42%] [G loss: 7.958364, adv: 1.561363, recon: 0.217039, id: 0.163097] time: 0:01:39.874396 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 42/995] [D loss: 1.244029, acc:  10%] [G loss: 8.968712, adv: 0.438601, recon: 0.370431, id: 0.355667] time: 0:01:40.372095 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 43/995] [D loss: 0.969303, acc:  60%] [G loss: 6.794332, adv: 0.867811, recon: 0.227399, id: 0.227698] time: 0:01:40.990783 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 44/995] [D loss: 0.760312, acc:  18%] [G loss: 7.749010, adv: 0.556746, recon: 0.302299, id: 0.181343] time: 0:01:41.526149 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 45/995] [D loss: 0.447624, acc:  31%] [G loss: 6.289020, adv: 0.635890, recon: 0.227740, id: 0.194343] time: 0:01:42.100779 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 46/995] [D loss: 0.688713, acc:  29%] [G loss: 9.078117, adv: 0.823824, recon: 0.340253, id: 0.474195] time: 0:01:42.712340 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 47/995] [D loss: 0.542100, acc:  17%] [G loss: 8.473968, adv: 0.532534, recon: 0.338891, id: 0.353757] time: 0:01:43.294697 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 48/995] [D loss: 0.728972, acc:   8%] [G loss: 5.615374, adv: 0.438566, recon: 0.216327, id: 0.164675] time: 0:01:44.012353 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 49/995] [D loss: 0.437780, acc:  37%] [G loss: 7.224516, adv: 0.624887, recon: 0.272322, id: 0.248775] time: 0:01:44.620286 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 50/995] [D loss: 0.771227, acc:  14%] [G loss: 7.441338, adv: 0.576389, recon: 0.283779, id: 0.239834] time: 0:01:45.234021 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 51/995] [D loss: 0.504752, acc:  35%] [G loss: 6.054148, adv: 1.080123, recon: 0.176340, id: 0.257602] time: 0:01:45.817645 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 52/995] [D loss: 0.673862, acc:  40%] [G loss: 6.569261, adv: 0.918436, recon: 0.215792, id: 0.257014] time: 0:01:46.411505 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 53/995] [D loss: 0.801121, acc:  29%] [G loss: 6.419631, adv: 0.443380, recon: 0.253890, id: 0.253740] time: 0:01:47.070696 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 54/995] [D loss: 0.520067, acc:  31%] [G loss: 7.449745, adv: 0.774889, recon: 0.256365, id: 0.394215] time: 0:01:47.612278 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 55/995] [D loss: 0.442554, acc:  28%] [G loss: 8.347799, adv: 0.683392, recon: 0.316621, id: 0.197019] time: 0:01:48.125671 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 56/995] [D loss: 0.611921, acc:  23%] [G loss: 8.925821, adv: 0.749424, recon: 0.337927, id: 0.376989] time: 0:01:48.810278 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 57/995] [D loss: 0.575848, acc:  31%] [G loss: 6.786680, adv: 0.683493, recon: 0.243839, id: 0.308040] time: 0:01:49.435181 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 58/995] [D loss: 0.728477, acc:  20%] [G loss: 9.763876, adv: 0.953697, recon: 0.360258, id: 0.281395] time: 0:01:50.146364 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 59/995] [D loss: 0.624710, acc:  32%] [G loss: 8.002466, adv: 0.571157, recon: 0.314031, id: 0.259826] time: 0:01:50.859924 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 60/995] [D loss: 0.279604, acc:  73%] [G loss: 7.613074, adv: 1.171498, recon: 0.238229, id: 0.230760] time: 0:01:51.533241 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 61/995] [D loss: 1.082510, acc:  16%] [G loss: 6.512033, adv: 0.722894, recon: 0.227832, id: 0.317502] time: 0:01:52.123170 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 62/995] [D loss: 0.690415, acc:  42%] [G loss: 8.193420, adv: 0.670319, recon: 0.311754, id: 0.316281] time: 0:01:52.693806 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 63/995] [D loss: 0.616260, acc:  33%] [G loss: 6.830615, adv: 0.875138, recon: 0.231317, id: 0.166180] time: 0:01:53.224981 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 64/995] [D loss: 1.355321, acc:  15%] [G loss: 7.426067, adv: 0.624283, recon: 0.275544, id: 0.376192] time: 0:01:53.824361 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 65/995] [D loss: 0.511998, acc:  37%] [G loss: 6.008774, adv: 0.964973, recon: 0.176376, id: 0.195958] time: 0:01:54.507236 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 66/995] [D loss: 0.578815, acc:  22%] [G loss: 9.299658, adv: 0.590272, recon: 0.362281, id: 0.425020] time: 0:01:55.091688 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 67/995] [D loss: 0.279594, acc:  49%] [G loss: 7.301294, adv: 0.824700, recon: 0.248952, id: 0.362079] time: 0:01:55.687322 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 68/995] [D loss: 0.357870, acc:  48%] [G loss: 5.992797, adv: 0.662369, recon: 0.203632, id: 0.283460] time: 0:01:56.310995 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 69/995] [D loss: 0.416390, acc:  36%] [G loss: 6.110139, adv: 0.793800, recon: 0.194093, id: 0.332086] time: 0:01:57.057192 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 70/995] [D loss: 0.525989, acc:  24%] [G loss: 9.136740, adv: 0.919058, recon: 0.325038, id: 0.459382] time: 0:01:57.612969 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 71/995] [D loss: 1.172505, acc:  10%] [G loss: 4.948289, adv: 0.299674, recon: 0.191034, id: 0.290787] time: 0:01:58.261387 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 72/995] [D loss: 0.573701, acc:  21%] [G loss: 5.289853, adv: 0.655842, recon: 0.179952, id: 0.198976] time: 0:01:58.975074 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 73/995] [D loss: 0.621117, acc:  46%] [G loss: 5.596262, adv: 0.684877, recon: 0.187501, id: 0.226843] time: 0:01:59.780496 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 74/995] [D loss: 0.505569, acc:  24%] [G loss: 6.303542, adv: 0.568508, recon: 0.234225, id: 0.274994] time: 0:02:00.402553 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 75/995] [D loss: 0.364937, acc:  44%] [G loss: 8.675318, adv: 0.803885, recon: 0.319786, id: 0.308738] time: 0:02:00.961697 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 76/995] [D loss: 0.522205, acc:  30%] [G loss: 6.992392, adv: 0.673096, recon: 0.255454, id: 0.223246] time: 0:02:01.574928 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 77/995] [D loss: 0.328555, acc:  40%] [G loss: 8.061857, adv: 0.706737, recon: 0.302612, id: 0.420545] time: 0:02:02.303061 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 78/995] [D loss: 0.750876, acc:  10%] [G loss: 6.985358, adv: 0.640367, recon: 0.260543, id: 0.206021] time: 0:02:02.865505 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 79/995] [D loss: 0.625978, acc:  25%] [G loss: 7.563732, adv: 0.712890, recon: 0.279022, id: 0.284162] time: 0:02:03.528457 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 80/995] [D loss: 1.189261, acc:   3%] [G loss: 6.440967, adv: 0.450481, recon: 0.245066, id: 0.348211] time: 0:02:04.115398 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 81/995] [D loss: 0.424440, acc:  33%] [G loss: 7.063907, adv: 0.516871, recon: 0.277327, id: 0.283901] time: 0:02:04.690610 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 82/995] [D loss: 0.491869, acc:  36%] [G loss: 6.376362, adv: 0.624324, recon: 0.232856, id: 0.235169] time: 0:02:05.185979 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 83/995] [D loss: 0.316060, acc:  79%] [G loss: 5.746459, adv: 1.090282, recon: 0.162384, id: 0.112210] time: 0:02:05.893835 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 84/995] [D loss: 0.707066, acc:  33%] [G loss: 8.094426, adv: 0.729540, recon: 0.300770, id: 0.308364] time: 0:02:06.383858 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 85/995] [D loss: 0.435114, acc:  75%] [G loss: 7.756966, adv: 1.401730, recon: 0.224682, id: 0.199941] time: 0:02:06.991953 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 86/995] [D loss: 0.288680, acc:  75%] [G loss: 6.966136, adv: 0.751139, recon: 0.249541, id: 0.320431] time: 0:02:07.603800 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 87/995] [D loss: 0.353740, acc:  46%] [G loss: 5.480494, adv: 0.648212, recon: 0.191856, id: 0.163929] time: 0:02:08.143854 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 88/995] [D loss: 0.690793, acc:  26%] [G loss: 6.995869, adv: 0.675928, recon: 0.259732, id: 0.272366] time: 0:02:08.718283 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 89/995] [D loss: 0.522062, acc:  23%] [G loss: 7.194594, adv: 0.719509, recon: 0.264203, id: 0.267659] time: 0:02:09.352332 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 90/995] [D loss: 0.475146, acc:  23%] [G loss: 6.446277, adv: 0.518347, recon: 0.246150, id: 0.223713] time: 0:02:10.000828 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 91/995] [D loss: 0.801403, acc:   6%] [G loss: 5.349844, adv: 0.589448, recon: 0.189844, id: 0.217038] time: 0:02:10.575018 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 92/995] [D loss: 0.557527, acc:  16%] [G loss: 7.018040, adv: 0.627976, recon: 0.260937, id: 0.370616] time: 0:02:11.173798 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 93/995] [D loss: 0.482782, acc:  14%] [G loss: 6.090538, adv: 0.608249, recon: 0.222257, id: 0.254961] time: 0:02:11.812340 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 94/995] [D loss: 0.540542, acc:  75%] [G loss: 5.322554, adv: 1.429337, recon: 0.110612, id: 0.085346] time: 0:02:12.580301 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 95/995] [D loss: 0.641022, acc:  28%] [G loss: 5.077607, adv: 0.517182, recon: 0.185124, id: 0.117852] time: 0:02:13.538585 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 96/995] [D loss: 0.573204, acc:  55%] [G loss: 6.902941, adv: 1.040062, recon: 0.222044, id: 0.155435] time: 0:02:14.214884 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 97/995] [D loss: 0.655238, acc:  21%] [G loss: 7.664581, adv: 0.659069, recon: 0.288722, id: 0.258834] time: 0:02:14.777052 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 98/995] [D loss: 0.365380, acc:  45%] [G loss: 7.637417, adv: 0.806658, recon: 0.272141, id: 0.368834] time: 0:02:15.468059 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 99/995] [D loss: 0.478936, acc:  37%] [G loss: 7.709424, adv: 0.698819, recon: 0.285206, id: 0.397607] time: 0:02:16.103034 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 100/995] [D loss: 0.839410, acc:  20%] [G loss: 8.279700, adv: 0.770192, recon: 0.309208, id: 0.263143] time: 0:02:16.815670 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 101/995] [D loss: 0.442977, acc:  35%] [G loss: 6.903091, adv: 0.603357, recon: 0.260343, id: 0.242563] time: 0:02:17.418229 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 102/995] [D loss: 0.888820, acc:  14%] [G loss: 7.815987, adv: 0.764091, recon: 0.284551, id: 0.281850] time: 0:02:18.332843 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 103/995] [D loss: 0.414199, acc:  39%] [G loss: 6.807953, adv: 0.785505, recon: 0.235654, id: 0.341249] time: 0:02:19.443938 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 104/995] [D loss: 0.393484, acc:  24%] [G loss: 4.654101, adv: 0.567689, recon: 0.160369, id: 0.160939] time: 0:02:20.668015 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 105/995] [D loss: 0.191307, acc:  71%] [G loss: 7.230791, adv: 1.003218, recon: 0.246529, id: 0.105493] time: 0:02:21.385586 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 106/995] [D loss: 0.718487, acc:  19%] [G loss: 6.686186, adv: 0.628582, recon: 0.251856, id: 0.264949] time: 0:02:21.932131 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 107/995] [D loss: 0.528668, acc:  24%] [G loss: 8.524040, adv: 0.545317, recon: 0.341658, id: 0.279541] time: 0:02:22.588098 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 108/995] [D loss: 0.575966, acc:  14%] [G loss: 7.153141, adv: 0.436212, recon: 0.289567, id: 0.351228] time: 0:02:23.120467 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 109/995] [D loss: 0.447193, acc:  21%] [G loss: 5.792946, adv: 0.535795, recon: 0.218335, id: 0.156934] time: 0:02:23.751437 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 110/995] [D loss: 0.441458, acc:  34%] [G loss: 6.020140, adv: 0.568267, recon: 0.224973, id: 0.139234] time: 0:02:24.269973 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 111/995] [D loss: 0.573118, acc:   7%] [G loss: 7.754356, adv: 0.486804, recon: 0.311850, id: 0.352920] time: 0:02:24.832504 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 112/995] [D loss: 0.519051, acc:  13%] [G loss: 7.733744, adv: 0.613868, recon: 0.299752, id: 0.303541] time: 0:02:25.484972 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 113/995] [D loss: 0.427544, acc:  25%] [G loss: 7.055416, adv: 0.577293, recon: 0.273193, id: 0.227425] time: 0:02:26.122171 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 114/995] [D loss: 0.372999, acc:  43%] [G loss: 4.558501, adv: 0.707263, recon: 0.143811, id: 0.131961] time: 0:02:26.744907 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 115/995] [D loss: 0.961319, acc:  23%] [G loss: 7.355142, adv: 0.874654, recon: 0.254558, id: 0.281217] time: 0:02:27.493675 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 116/995] [D loss: 0.853070, acc:  37%] [G loss: 5.042991, adv: 0.326840, recon: 0.199211, id: 0.236311] time: 0:02:28.035879 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 117/995] [D loss: 0.445039, acc:  38%] [G loss: 5.322244, adv: 0.730954, recon: 0.174908, id: 0.192721] time: 0:02:28.768898 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 118/995] [D loss: 0.541918, acc:   5%] [G loss: 6.239054, adv: 0.599918, recon: 0.229167, id: 0.256799] time: 0:02:29.781910 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 119/995] [D loss: 0.509018, acc:  17%] [G loss: 7.119752, adv: 0.534010, recon: 0.273733, id: 0.364329] time: 0:02:30.852107 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 120/995] [D loss: 0.388789, acc:  37%] [G loss: 5.296722, adv: 0.686886, recon: 0.173555, id: 0.256938] time: 0:02:31.726825 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 121/995] [D loss: 0.490886, acc:  35%] [G loss: 5.117121, adv: 0.527764, recon: 0.188623, id: 0.100758] time: 0:02:32.348257 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 122/995] [D loss: 0.421780, acc:  32%] [G loss: 6.580102, adv: 0.551998, recon: 0.250083, id: 0.252116] time: 0:02:33.049502 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 123/995] [D loss: 0.797174, acc:   9%] [G loss: 5.737175, adv: 0.456073, recon: 0.217686, id: 0.290022] time: 0:02:33.728262 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 124/995] [D loss: 0.422836, acc:  30%] [G loss: 3.261336, adv: 0.429017, recon: 0.106538, id: 0.144064] time: 0:02:34.378968 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 125/995] [D loss: 0.414600, acc:  37%] [G loss: 5.453182, adv: 0.514246, recon: 0.200577, id: 0.154376] time: 0:02:34.874294 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 126/995] [D loss: 0.491507, acc:  30%] [G loss: 6.029489, adv: 0.855667, recon: 0.193887, id: 0.242534] time: 0:02:35.397715 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 127/995] [D loss: 0.347614, acc:  52%] [G loss: 5.975792, adv: 0.677533, recon: 0.211087, id: 0.163903] time: 0:02:35.940988 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 128/995] [D loss: 0.590146, acc:  15%] [G loss: 4.640449, adv: 0.367397, recon: 0.176783, id: 0.203832] time: 0:02:36.710440 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 129/995] [D loss: 0.477870, acc:  29%] [G loss: 4.097888, adv: 0.459705, recon: 0.139328, id: 0.191099] time: 0:02:37.257722 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 130/995] [D loss: 0.433110, acc:  25%] [G loss: 5.381768, adv: 0.494089, recon: 0.194588, id: 0.257840] time: 0:02:37.927627 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 131/995] [D loss: 0.413625, acc:  23%] [G loss: 5.385745, adv: 0.606563, recon: 0.186419, id: 0.218352] time: 0:02:38.630756 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 132/995] [D loss: 0.480353, acc:  30%] [G loss: 3.923091, adv: 0.396444, recon: 0.140828, id: 0.160980] time: 0:02:39.358594 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 133/995] [D loss: 0.467196, acc:  29%] [G loss: 6.808110, adv: 0.451655, recon: 0.265809, id: 0.313294] time: 0:02:39.976433 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 134/995] [D loss: 0.327533, acc:  36%] [G loss: 3.578139, adv: 0.499049, recon: 0.115706, id: 0.123683] time: 0:02:40.604960 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 135/995] [D loss: 0.665487, acc:   3%] [G loss: 4.815502, adv: 0.364091, recon: 0.185117, id: 0.178176] time: 0:02:41.151683 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 136/995] [D loss: 0.302054, acc:  46%] [G loss: 4.913707, adv: 0.660982, recon: 0.158134, id: 0.100314] time: 0:02:41.641649 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 137/995] [D loss: 0.444122, acc:  32%] [G loss: 5.301592, adv: 0.483085, recon: 0.189924, id: 0.268370] time: 0:02:42.254192 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 138/995] [D loss: 0.517767, acc:  17%] [G loss: 8.197556, adv: 0.573660, recon: 0.321734, id: 0.277607] time: 0:02:42.822506 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 139/995] [D loss: 0.432954, acc:  28%] [G loss: 6.740236, adv: 0.597795, recon: 0.251371, id: 0.236757] time: 0:02:43.426724 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 140/995] [D loss: 0.512305, acc:  12%] [G loss: 5.793117, adv: 0.579823, recon: 0.208582, id: 0.263914] time: 0:02:44.120072 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 141/995] [D loss: 0.483267, acc:  26%] [G loss: 4.844007, adv: 0.527054, recon: 0.168450, id: 0.183460] time: 0:02:44.922940 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 142/995] [D loss: 0.419684, acc:  47%] [G loss: 5.964004, adv: 0.520958, recon: 0.223307, id: 0.322490] time: 0:02:45.474395 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 143/995] [D loss: 0.461465, acc:  33%] [G loss: 5.184223, adv: 0.496634, recon: 0.190097, id: 0.167714] time: 0:02:46.023627 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 144/995] [D loss: 0.423773, acc:  32%] [G loss: 6.115926, adv: 0.827419, recon: 0.198538, id: 0.287983] time: 0:02:46.691889 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 145/995] [D loss: 0.392433, acc:  42%] [G loss: 5.901170, adv: 0.515206, recon: 0.215712, id: 0.267188] time: 0:02:47.232711 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 146/995] [D loss: 0.543997, acc:  24%] [G loss: 6.993334, adv: 0.508641, recon: 0.268075, id: 0.303087] time: 0:02:47.844423 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 147/995] [D loss: 0.359543, acc:  44%] [G loss: 6.259439, adv: 0.557245, recon: 0.232491, id: 0.219433] time: 0:02:48.470779 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 148/995] [D loss: 0.316506, acc:  45%] [G loss: 4.630599, adv: 0.594313, recon: 0.158861, id: 0.151823] time: 0:02:49.039815 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 149/995] [D loss: 0.376462, acc:  41%] [G loss: 6.846109, adv: 0.561819, recon: 0.261955, id: 0.215224] time: 0:02:49.660018 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 150/995] [D loss: 0.131699, acc:  84%] [G loss: 4.931755, adv: 0.804402, recon: 0.150506, id: 0.190297] time: 0:02:50.360690 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 151/995] [D loss: 0.461923, acc:  30%] [G loss: 5.455869, adv: 0.463041, recon: 0.207439, id: 0.250316] time: 0:02:50.981772 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 152/995] [D loss: 0.256568, acc:  52%] [G loss: 4.574358, adv: 0.751156, recon: 0.139767, id: 0.154596] time: 0:02:51.600004 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 153/995] [D loss: 0.232686, acc:  66%] [G loss: 7.732994, adv: 0.584026, recon: 0.299673, id: 0.330021] time: 0:02:52.195290 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 154/995] [D loss: 0.315195, acc:  46%] [G loss: 6.884672, adv: 0.611029, recon: 0.256082, id: 0.222725] time: 0:02:53.364850 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 155/995] [D loss: 0.586104, acc:   5%] [G loss: 5.821547, adv: 0.450959, recon: 0.218396, id: 0.331816] time: 0:02:54.019046 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 156/995] [D loss: 0.302129, acc:  44%] [G loss: 7.129982, adv: 0.699068, recon: 0.262237, id: 0.317697] time: 0:02:54.604766 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 157/995] [D loss: 0.622550, acc:   8%] [G loss: 8.449069, adv: 0.666657, recon: 0.324291, id: 0.312387] time: 0:02:55.271936 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 158/995] [D loss: 0.483731, acc:  30%] [G loss: 8.969755, adv: 0.593781, recon: 0.357085, id: 0.422806] time: 0:02:55.817201 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 159/995] [D loss: 0.410291, acc:  45%] [G loss: 6.609173, adv: 0.758901, recon: 0.228680, id: 0.160907] time: 0:02:56.423122 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 160/995] [D loss: 0.563349, acc:  25%] [G loss: 5.652096, adv: 0.714570, recon: 0.188635, id: 0.223447] time: 0:02:57.074606 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 161/995] [D loss: 0.403577, acc:  32%] [G loss: 4.868564, adv: 0.778510, recon: 0.145523, id: 0.177317] time: 0:02:57.693332 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 162/995] [D loss: 0.522619, acc:  29%] [G loss: 6.707902, adv: 0.455714, recon: 0.270484, id: 0.206973] time: 0:02:58.332714 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 163/995] [D loss: 0.469525, acc:  28%] [G loss: 5.947410, adv: 0.446416, recon: 0.227566, id: 0.221354] time: 0:02:58.877663 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 164/995] [D loss: 0.289026, acc:  45%] [G loss: 4.844525, adv: 0.607514, recon: 0.163437, id: 0.098422] time: 0:02:59.378355 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 165/995] [D loss: 0.301079, acc:  57%] [G loss: 5.397342, adv: 0.822871, recon: 0.168457, id: 0.157524] time: 0:03:00.037925 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 166/995] [D loss: 0.676524, acc:  15%] [G loss: 5.152997, adv: 0.483430, recon: 0.188667, id: 0.187236] time: 0:03:00.586752 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 167/995] [D loss: 0.410557, acc:  30%] [G loss: 5.675522, adv: 0.663967, recon: 0.197191, id: 0.201765] time: 0:03:01.142976 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 168/995] [D loss: 0.548822, acc:  17%] [G loss: 6.063703, adv: 0.398272, recon: 0.241847, id: 0.154517] time: 0:03:01.678755 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 169/995] [D loss: 0.493485, acc:  28%] [G loss: 5.006859, adv: 0.461297, recon: 0.187002, id: 0.180229] time: 0:03:02.372526 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 170/995] [D loss: 0.401092, acc:  26%] [G loss: 3.418642, adv: 0.459571, recon: 0.114184, id: 0.119814] time: 0:03:02.922402 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 171/995] [D loss: 0.457867, acc:  19%] [G loss: 4.951680, adv: 0.723980, recon: 0.159473, id: 0.168493] time: 0:03:03.609034 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 172/995] [D loss: 0.341852, acc:  53%] [G loss: 3.666393, adv: 0.574507, recon: 0.113560, id: 0.122373] time: 0:03:04.237795 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 173/995] [D loss: 0.728968, acc:  13%] [G loss: 4.588589, adv: 0.507820, recon: 0.159028, id: 0.220790] time: 0:03:04.788459 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 174/995] [D loss: 0.307179, acc:  59%] [G loss: 5.554417, adv: 0.811319, recon: 0.178766, id: 0.143742] time: 0:03:05.360562 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 175/995] [D loss: 0.375999, acc:  40%] [G loss: 5.590055, adv: 0.607014, recon: 0.200359, id: 0.224802] time: 0:03:05.928842 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 176/995] [D loss: 0.316618, acc:  51%] [G loss: 6.126792, adv: 0.808257, recon: 0.206590, id: 0.167032] time: 0:03:06.438908 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 177/995] [D loss: 0.608567, acc:  25%] [G loss: 4.714767, adv: 0.486533, recon: 0.167212, id: 0.184530] time: 0:03:07.181757 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 178/995] [D loss: 0.311816, acc:  43%] [G loss: 4.972718, adv: 0.706816, recon: 0.157582, id: 0.208434] time: 0:03:07.787984 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 179/995] [D loss: 0.530805, acc:  36%] [G loss: 5.994208, adv: 0.878272, recon: 0.191900, id: 0.242796] time: 0:03:08.365822 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 180/995] [D loss: 0.703794, acc:   8%] [G loss: 4.978002, adv: 0.560966, recon: 0.171154, id: 0.171448] time: 0:03:08.963613 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 181/995] [D loss: 0.372455, acc:  47%] [G loss: 5.730243, adv: 0.625276, recon: 0.202903, id: 0.212762] time: 0:03:09.526224 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 182/995] [D loss: 0.469811, acc:  28%] [G loss: 5.204456, adv: 0.494944, recon: 0.186960, id: 0.228427] time: 0:03:10.138380 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 183/995] [D loss: 0.452205, acc:  21%] [G loss: 4.819023, adv: 0.606981, recon: 0.159162, id: 0.187015] time: 0:03:10.669673 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 184/995] [D loss: 0.463791, acc:  26%] [G loss: 4.610617, adv: 0.481584, recon: 0.163189, id: 0.207594] time: 0:03:11.269688 \n",
            "mm\n",
            "[Epoch 0/200] [Batch 185/995] [D loss: 0.268430, acc:  60%] [G loss: 4.514382, adv: 0.604460, recon: 0.147788, id: 0.111454] time: 0:03:11.875258 \n",
            "mm\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7576cdbc48c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-24e258469f91>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;31m# Train the discriminators (original images = real / translated = Fake)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mdA_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m                 \u001b[0mdA_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mdA_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdA_loss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1900\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1902\u001b[0;31m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1903\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mndim\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   3073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ndim_dispatcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3076\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m     \"\"\"\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}